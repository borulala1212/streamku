# -*- coding: utf-8 -*-
"""Untitled26.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JBbXXdP3Me7WnvYLD6KUmAuO43WVmlR5
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

data = pd.read_csv("Diabetes.csv")
data

data.columns

#checking data
data.isnull().sum()

data.describe()

data.info()

correlation_matrix=data.corr().round(2)
correlation_matrix["Outcome"].sort_values(ascending=False)

# Plot Correlation with Heatmap
plt.figure(figsize=(20,10))
sns.heatmap(data.corr(),annot=True)
plt.show()

X = data.drop(["Outcome"],axis=1)
y = data["Outcome"]

no_disease = data[data["Outcome"]==0]
disease = data[data["Outcome"]==1]

print(no_disease.shape,disease.shape)

from imblearn.over_sampling import RandomOverSampler
os = RandomOverSampler(random_state=10)

X_res,y_res=os.fit_resample(X,y)

X_res.shape,y_res.shape

"""## RandomForest

"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X_res, y_res, test_size=0.2,random_state=10)

from sklearn.ensemble import RandomForestClassifier
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import GridSearchCV
param_grid={'n_estimators':range(80,201,5),'criterion':['gini','entropy'],'max_features':['auto','sqrt','log2',None]}

tuning = GridSearchCV(estimator=RandomForestClassifier(),param_grid=param_grid,cv=5,verbose=2,n_jobs=-1,scoring='f1')
tuning.fit(X_train,y_train)
tuning.best_params_,tuning.best_score_

classifierRF = RandomForestClassifier(n_estimators=85,criterion='gini',random_state=10,max_features='sqrt')

model = classifierRF.fit(X_train,y_train)
y_pred = model.predict(X_test)

from sklearn.model_selection import cross_val_score
score = cross_val_score(model,X_train,y_train,cv=10)

#Random forest classifier
print("Maximum Accuracy : ",round(max(score)*100,2),"%")
print("Average Accuracy : ",round(score.mean()*100,2),"%")
print("Average Deviation : ",round(score.std()*100,2),"%")

from sklearn.metrics import roc_auc_score
print(roc_auc_score(y_test,y_pred))

from sklearn.metrics import plot_roc_curve
plot_roc_curve(model,X_test,y_test)

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix

print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

# Using Joblib
import joblib

# Save Decision Tree Model
model_file_rf = open("RandomForest_model_diabetes.pkl","wb")
joblib.dump(classifierRF,model_file_rf)
model_file_rf.close()

"""## Logistic Reggresion

"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2,random_state=10)

X_tr = X_train
X_te = X_test

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_tr = sc.fit_transform(X_tr)
X_te = sc.transform(X_te)

from sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings('ignore')

param_grid = [{'max_iter' : range(1,50,1),'solver' : ['liblinear','saga','newton_cg','sag','lbfgs']}]

from sklearn.model_selection import GridSearchCV
tuning = GridSearchCV(estimator=LogisticRegression(),param_grid=param_grid,cv=5,verbose=True,n_jobs=-1,scoring='f1')

tuning.fit(X_train,y_train)
tuning.best_params_,tuning.best_score_

model=LogisticRegression(max_iter=12,solver='newton-cg',random_state=10)
model.fit(X_tr, y_train)
y_pred = model.predict(X_te)

from numpy.core.numeric import cross
from sklearn.model_selection import cross_val_score
score1=cross_val_score(model,X_tr,y_train,cv=10)
score1

#Logistik classifier
print("Maximum Accuracy : ",round(max(score1)*100,2),"%")
print("Average Accuracy : ",round(score1.mean()*100,2),"%")
print("Average Deviation : ",round(score1.std()*100,2),"%")

from sklearn.metrics import roc_auc_score, plot_roc_curve
print(roc_auc_score(y_test,y_pred))

plot_roc_curve(model,X_te,y_test)

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix

print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""KNN"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X_res,y_res,test_size=0.2,random_state=10)

from sklearn.preprocessing import StandardScaler
X_tr = X_train
X_te = X_test 
sc = StandardScaler()
X_tr = sc.fit_transform(X_tr)
X_te = sc.transform(X_te)

param_grid = [{'n_neighbors' : range(0,20,1),'algorithm':['auto','ball_tree','kd_tree','brute'],'weights':['uniform','distance'],'p':[0,1,2,3]}]

from sklearn.neighbors import KNeighborsClassifier

tuning = GridSearchCV(estimator=KNeighborsClassifier(),param_grid=param_grid,cv=5,verbose=2,n_jobs=-1,scoring='f1')
tuning.fit(X_tr,y_train)
tuning.best_params_,tuning.best_score_

classifier = KNeighborsClassifier(n_neighbors=1,p=2,algorithm='auto',weights='uniform')
classifier.fit(X_tr,y_train)
y_pred=classifier.predict(X_te)
print(y_pred)
print(y_test)

score2 = cross_val_score(classifier,X_tr,y_train,cv=10)
score2

#KNN classifier
print("Maximum Accuracy : ",round(max(score2)*100,2),"%")
print("Average Accuracy : ",round(score2.mean()*100,2),"%")
print("Average Deviation : ",round(score2.std()*100,2),"%")

print(roc_auc_score(y_test,y_pred))

plot_roc_curve(classifier,X_te,y_test)

"""Decision tree

"""

from sklearn.tree import DecisionTreeClassifier
dt_model = DecisionTreeClassifier()

dt_model.fit(X_tr,y_train)

# Check Model Accuracy
y_pred2 = dt_model.predict(X_te)
# Using Accuracy Score to check for accuracy by comparing with the predicted values and the test values
print("Accuracy of Decision Tree Model:{}".format(accuracy_score(y_test,y_pred2)))

# Classification Report For Decision Tree Model
print(classification_report(y_test,y_pred2))